[
  
  {
    "title": "Video-LLaMA",
    "url": "/posts/Video-LLaMA/",
    "categories": "EMNLP 2023",
    "tags": "MM-LLMs",
    "date": "2023-06-05 01:00:00 +0900",
    





    
    "snippet": "  Paper: Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding  GitHub Link  Publisher: EMNLP 2023  Author Affiliation: Alibaba Group  Functional Division          U...",
    "content": "  Paper: Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding  GitHub Link  Publisher: EMNLP 2023  Author Affiliation: Alibaba Group  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+V+A+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I/V: Eva-CLIP ViT-G/14          A: ImageBind                    Input Projector                  Q-Former w/ Linear Projector                    LLM Backbone                  Vicuna/LLaMA                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "LLaVA-Med",
    "url": "/posts/LLaVA-Med/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-06-01 01:00:00 +0900",
    





    
    "snippet": "  Paper: LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft  Functional Division          Understandi...",
    "content": "  Paper: LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T      "
  },
  
  {
    "title": "PaLI-X",
    "url": "/posts/PaLI-X/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-05-29 01:00:00 +0900",
    





    
    "snippet": "  Paper: PaLI-X: On scaling up a multilingual vision and language model  GitHub Link: None  Publisher: Arxiv  Author Affiliation: Google Research  Functional Division          Understanding      Ge...",
    "content": "  Paper: PaLI-X: On scaling up a multilingual vision and language model  GitHub Link: None  Publisher: Arxiv  Author Affiliation: Google Research  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: ViT                    Input Projector                  Linear Projector                    LLM Backbone                  UL2-32B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "GILL",
    "url": "/posts/GILL/",
    "categories": "NIPS 2023",
    "tags": "MM-LLMs",
    "date": "2023-05-26 01:00:00 +0900",
    





    
    "snippet": "  Paper: Generating Images with Multimodal Language Models  GitHub Link  Publisher: NIPS 2023  Author Affiliation: Carnegie Mellon University  Functional Division          Understanding      Genera...",
    "content": "  Paper: Generating Images with Multimodal Language Models  GitHub Link  Publisher: NIPS 2023  Author Affiliation: Carnegie Mellon University  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ I+T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: CLIP ViT-L                    Input Projector                  Linear Projector                    LLM Backbone                  OPT-6.7B                    Output Projector                  Tiny Transformer                    Modality Generator                  I: Stable Diffusion-1.5                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "PandaGPT",
    "url": "/posts/PandaGPT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-05-25 01:00:00 +0900",
    





    
    "snippet": "  Paper: PandaGPT: One Model To Instruction-Follow Them All  GitHub Link  Publisher: Arxiv  Author Affiliation: University of Cambridge  Functional Division          Understanding      Generation  ...",
    "content": "  Paper: PandaGPT: One Model To Instruction-Follow Them All  GitHub Link  Publisher: Arxiv  Author Affiliation: University of Cambridge  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: ImageBind                    Input Projector                  Linear Projector                    LLM Backbone                  Vicuna-13B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "EmbodiedGPT",
    "url": "/posts/EmbodiedGPT/",
    "categories": "NIPS 2023",
    "tags": "MM-LLMs",
    "date": "2023-05-24 01:00:00 +0900",
    





    
    "snippet": "  Paper: EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought  GitHub Link  Publisher: NIPS 2023  Author Affiliation: The University of Hong Kong,  Functional Division          U...",
    "content": "  Paper: EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought  GitHub Link  Publisher: NIPS 2023  Author Affiliation: The University of Hong Kong,  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+V+T $\\rightarrow$ T      "
  },
  
  {
    "title": "DetGPT",
    "url": "/posts/DetGPT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-05-23 01:00:00 +0900",
    





    
    "snippet": "  Paper: DetGPT: Detect What You Need via Reasoning  GitHub Link  Publisher: Arxiv  Author Affiliation: The Hong Kong University of Science and Technology  Functional Division          Understandin...",
    "content": "  Paper: DetGPT: Detect What You Need via Reasoning  GitHub Link  Publisher: Arxiv  Author Affiliation: The Hong Kong University of Science and Technology  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ IB+T      "
  },
  
  {
    "title": "SpeechGPT",
    "url": "/posts/SpeechGPT/",
    "categories": "EMNLP 2023",
    "tags": "MM-LLMs",
    "date": "2023-05-18 01:00:00 +0900",
    





    
    "snippet": "  Paper: SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities  GitHub Link  Publisher: EMNLP 2023  Author Affiliation: Fudan University  Functional Divisi...",
    "content": "  Paper: SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities  GitHub Link  Publisher: EMNLP 2023  Author Affiliation: Fudan University  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          A+T $\\rightarrow$ A+T      "
  },
  
  {
    "title": "InstructBLIP",
    "url": "/posts/InstructBLIP/",
    "categories": "NIPS 2023",
    "tags": "MM-LLMs",
    "date": "2023-05-11 01:00:00 +0900",
    





    
    "snippet": "  Paper: InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning  GitHub Link  Publisher: NIPS 2023  Author Affiliation: Salesforce Research  Functional Division       ...",
    "content": "  Paper: InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning  GitHub Link  Publisher: NIPS 2023  Author Affiliation: Salesforce Research  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+V+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I/V: ViT-G/14@224                    Input Projector                  Q-Former w/ Linear Projector                    LLM Backbone                  Flan-T5/Vicuna                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  129M                    Instruction-tuning Stage                  1.2M                    "
  },
  
  {
    "title": "VideoChat",
    "url": "/posts/VideoChat/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-05-10 01:00:00 +0900",
    





    
    "snippet": "  Paper: VideoChat: Chat-Centric Video Understanding  GitHub Link  Publisher: Arxiv  Author Affiliation: Shanghai AI Laboratory &amp; Nanjing University &amp; The University of Hong Kong &amp; Chin...",
    "content": "  Paper: VideoChat: Chat-Centric Video Understanding  GitHub Link  Publisher: Arxiv  Author Affiliation: Shanghai AI Laboratory &amp; Nanjing University &amp; The University of Hong Kong &amp; Chinese Academy of Sciences  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          V+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  V: ViT-G                    Input Projector                  Q-Former w/ Linear Projector                    LLM Backbone                  Vicuna                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "MultiModal-GPT",
    "url": "/posts/MultiModal-GPT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-05-08 01:00:00 +0900",
    





    
    "snippet": "  Paper: MultiModal-GPT: A Vision and Language Model for Dialogue with Humans  GitHub Link  Publisher: Arxiv  Author Affiliation: Shanghai AI Laboratory &amp; The University of Hong Kong &amp; Tian...",
    "content": "  Paper: MultiModal-GPT: A Vision and Language Model for Dialogue with Humans  GitHub Link  Publisher: Arxiv  Author Affiliation: Shanghai AI Laboratory &amp; The University of Hong Kong &amp; Tianjin University  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T      "
  },
  
  {
    "title": "X-LLM",
    "url": "/posts/X-LLM/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-05-07 01:00:00 +0900",
    





    
    "snippet": "  Paper: X-LLM:Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages  GitHub Link  Publisher: Arxiv  Author Affiliation: Chinese Academy of Sciences  Functi...",
    "content": "  Paper: X-LLM:Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages  GitHub Link  Publisher: Arxiv  Author Affiliation: Chinese Academy of Sciences  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+V+A+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I/V: ViT-G          A: C-Former                    Input Projector                  Q-Former w/ Linear Projector                    LLM Backbone                  ChatGLM-6B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "Otter",
    "url": "/posts/Otter/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-05-05 01:00:00 +0900",
    





    
    "snippet": "  Paper: Otter: A Multi-Modal Model with In-Context Instruction Tuning  GitHub Link  Publisher: Arxiv  Author Affiliation: Nanyang Technological University  Functional Division          Understandi...",
    "content": "  Paper: Otter: A Multi-Modal Model with In-Context Instruction Tuning  GitHub Link  Publisher: Arxiv  Author Affiliation: Nanyang Technological University  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: CLIP ViT-L/14                    Input Projector                  Cross-attention                    LLM Backbone                  LLaMA-7B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "mPLUG-Owl",
    "url": "/posts/mPLUG-Owl/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-04-27 01:00:00 +0900",
    





    
    "snippet": "  Paper: mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality  GitHub Link  Publisher: Arxiv  Author Affiliation: DAMO Academy, Alibaba Group  Functional Division          Un...",
    "content": "  Paper: mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality  GitHub Link  Publisher: Arxiv  Author Affiliation: DAMO Academy, Alibaba Group  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: CLIP ViT-L/14                    Input Projector                  Cross-attention                    LLM Backbone                  LLaMA-7B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "AudioGPT",
    "url": "/posts/AudioGPT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-04-25 01:00:00 +0900",
    





    
    "snippet": "  Paper: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head  GitHub Link  Publisher: Arxiv  Author Affiliation: Zhejiang University &amp; Peking University &amp; Carnegie...",
    "content": "  Paper: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head  GitHub Link  Publisher: Arxiv  Author Affiliation: Zhejiang University &amp; Peking University &amp; Carnegie Mellon University &amp; Remin University of China  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+A+T $\\rightarrow$ A+V+T      "
  },
  
  {
    "title": "MiniGPT-4",
    "url": "/posts/MiniGPT-4/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-04-20 01:00:00 +0900",
    





    
    "snippet": "  Paper: MINIGPT-4:ENHANCING VISION-LANGUAGE UNDERSTANDING WITH ADVANCED LARGE LANGUAGE MODELS  GitHub Link  Publisher: Arxiv  Author Affiliation: King Abdullah University of Science and Technology...",
    "content": "  Paper: MINIGPT-4:ENHANCING VISION-LANGUAGE UNDERSTANDING WITH ADVANCED LARGE LANGUAGE MODELS  GitHub Link  Publisher: Arxiv  Author Affiliation: King Abdullah University of Science and Technology  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: Eva-CLIP ViT-G/14                    Input Projector                  Q-Former w/ Linear Projector                    LLM Backbone                  Vicuna-13B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "LLaVA",
    "url": "/posts/LLaVA/",
    "categories": "NIPS 2023",
    "tags": "MM-LLMs",
    "date": "2023-04-17 01:00:00 +0900",
    





    
    "snippet": "  Paper: Visual Instruction Tuning  GitHub Link  Publisher: NIPS 2023  Author Affiliation: University of Wisconsinâ€“Madison &amp; Microsoft Research &amp; Columbia University  Functional Division   ...",
    "content": "  Paper: Visual Instruction Tuning  GitHub Link  Publisher: NIPS 2023  Author Affiliation: University of Wisconsinâ€“Madison &amp; Microsoft Research &amp; Columbia University  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: CLIP ViT-L/14                    Input Projector                  Linear Projector                    LLM Backbone                  Vicuna-7B/13B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "HuggingGPT",
    "url": "/posts/HuggingGPT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-03-30 01:00:00 +0900",
    





    
    "snippet": "  Paper: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face  GitHub Link  Publisher: Arxiv  Author Affiliation: Zhejiang University &amp; Microsoft Research Asia  Functional ...",
    "content": "  Paper: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face  GitHub Link  Publisher: Arxiv  Author Affiliation: Zhejiang University &amp; Microsoft Research Asia  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+V+A+T $\\rightarrow$ I+V+A+T      "
  },
  
  {
    "title": "MM-REACT",
    "url": "/posts/MM-REACT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-03-20 01:00:00 +0900",
    





    
    "snippet": "  Paper: MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft Azure AI  Functional Division          Understanding      Gene...",
    "content": "  Paper: MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft Azure AI  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+V+T $\\rightarrow$ T      "
  },
  
  {
    "title": "GPT-4",
    "url": "/posts/GPT-4/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-03-15 01:00:00 +0900",
    





    
    "snippet": "  Paper: GPT-4 Technical Report  GitHub Link: None  Publisher: Arxiv  Author Affiliation: OpenAI  Functional Division          Understanding      Generation        Design Division          Tool-usi...",
    "content": "  Paper: GPT-4 Technical Report  GitHub Link: None  Publisher: Arxiv  Author Affiliation: OpenAI  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T      "
  },
  
  {
    "title": "ViperGPT",
    "url": "/posts/ViperGPT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-03-14 01:00:00 +0900",
    





    
    "snippet": "  Paper: ViperGPT: Visual Inference via Python Execution for Reasoni  GitHub Link  Publisher: Arxiv  Author Affiliation: Columbia University  Functional Division          Understanding      Generat...",
    "content": "  Paper: ViperGPT: Visual Inference via Python Execution for Reasoni  GitHub Link  Publisher: Arxiv  Author Affiliation: Columbia University  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T      "
  },
  
  {
    "title": "Visual ChatGPT",
    "url": "/posts/Visual-ChatGPT/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-03-08 01:00:00 +0900",
    





    
    "snippet": "  Paper: Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft Research Asia  Functional Division          Underst...",
    "content": "  Paper: Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft Research Asia  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ I+T      "
  },
  
  {
    "title": "PaLM-E",
    "url": "/posts/PaLM-E/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-03-06 01:00:00 +0900",
    





    
    "snippet": "  Paper: PaLM-E: An Embodied Multimodal Language Model  GitHub Link  Publisher: Arxiv  Author Affiliation: Google  Functional Division          Understanding      Generation        Design Division ...",
    "content": "  Paper: PaLM-E: An Embodied Multimodal Language Model  GitHub Link  Publisher: Arxiv  Author Affiliation: Google  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T      "
  },
  
  {
    "title": "Kosmos-1",
    "url": "/posts/Kosmos-1/",
    "categories": "Arxiv",
    "tags": "MM-LLMs",
    "date": "2023-02-27 01:00:00 +0900",
    





    
    "snippet": "  Paper: Language Is Not All You Need: Aligning Perception with Language Models  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft  Functional Division          Understanding      Genera...",
    "content": "  Paper: Language Is Not All You Need: Aligning Perception with Language Models  GitHub Link  Publisher: Arxiv  Author Affiliation: Microsoft  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T      "
  },
  
  {
    "title": "FROMAGe",
    "url": "/posts/FROMAGe/",
    "categories": "ICML 2023",
    "tags": "MM-LLMs",
    "date": "2023-01-31 01:00:00 +0900",
    





    
    "snippet": "  Paper: Grounding Language Models to Images for Multimodal Inputs and Outputs  GitHub Link  Publisher: ICML 2023  Author Affiliation: Carnegie Mellon University  Functional Division          Under...",
    "content": "  Paper: Grounding Language Models to Images for Multimodal Inputs and Outputs  GitHub Link  Publisher: ICML 2023  Author Affiliation: Carnegie Mellon University  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ I+T      "
  },
  
  {
    "title": "BLIP-2",
    "url": "/posts/BLIP2/",
    "categories": "ICML 2023",
    "tags": "MM-LLMs",
    "date": "2023-01-30 01:00:00 +0900",
    





    
    "snippet": "  Paper: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models  GitHub Link  Publisher: ICML 2023  Author Affiliation: Salesforce Research  Function...",
    "content": "  Paper: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models  GitHub Link  Publisher: ICML 2023  Author Affiliation: Salesforce Research  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I: CLIP/Eva-CLIP ViT@224                    Input Projector                  Q-Former w/ Linear Projector                    LLM Backbone                  Flan-T5/OPT                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  129M                    Instruction-tuning Stage                  Not report                    "
  },
  
  {
    "title": "Flamingo",
    "url": "/posts/Flamingo/",
    "categories": "NIPS 2022",
    "tags": "MM-LLMs",
    "date": "2022-04-29 01:00:00 +0900",
    





    
    "snippet": "  Paper: ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning  GitHub Link: None  Publisher: NIPS 2022  Author Affiliation: DeepMind  Functional Division          Understanding      Generation ...",
    "content": "  Paper: ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning  GitHub Link: None  Publisher: NIPS 2022  Author Affiliation: DeepMind  Functional Division          Understanding      Generation        Design Division          Tool-using      End-to-end        Input Modalities $\\rightarrow$ Output Modalities (I: Image, V: Video, A: Audio, 3D: Point Cloud, T: Text)          I+V+T $\\rightarrow$ T        Model Architecture (Input $\\rightarrow$ Modality Encoder $\\rightarrow$ Input Projector $\\rightarrow$ LLM Backbone $\\rightarrow$ Output Projector $\\rightarrow$ Modality Generator $\\rightarrow$ Output)          Modality Encoder                  I/V: NFNet-F6                    Input Projector                  Cross-attention                    LLM Backbone                  Chinchilla-1.4B/7B/70B                    Output Projector                  None                    Modality Generator                  None                      Datasets Scale          Pre-training Stage                  Not report                    Instruction-tuning Stage                  Not report                    "
  }
  
]

